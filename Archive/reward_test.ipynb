{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "14cad018-5b82-4a60-b6dc-54f566569530",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14cad018-5b82-4a60-b6dc-54f566569530",
        "outputId": "3ecf4fef-b7b9-43ea-e725-46c119a310e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: clawpack in /usr/local/lib/python3.11/dist-packages (5.11.0)\n"
          ]
        }
      ],
      "source": [
        "pip install clawpack"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from clawpack import riemann, pyclaw\n",
        "import sys\n",
        "import os\n",
        "from google.colab import drive\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Mount Google Drive if you haven't already\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Assuming your project is in Google Drive\n",
        "# Adjust this path to point to your project's root directory\n",
        "project_path = '/content/drive/MyDrive/cs234_project'  # Change this to your actual path\n",
        "sys.path.append(project_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ektEsehSw_dc",
        "outputId": "617ade4f-4478-4644-8905-3317fa3bb3c0"
      },
      "id": "ektEsehSw_dc",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2052d327-bd43-42cf-b63f-7617d09b6b99",
      "metadata": {
        "id": "2052d327-bd43-42cf-b63f-7617d09b6b99"
      },
      "source": [
        "## 1. Mathematical Definition of the PDE\n",
        "\n",
        "The 2D advection equation describes the transport of a scalar quantity $q(x,y,t)$ by a velocity field $(u_x, u_y)$:\n",
        "\n",
        "$$\\frac{\\partial q}{\\partial t} + u_x \\frac{\\partial q}{\\partial x} + u_y \\frac{\\partial q}{\\partial y} = 0$$\n",
        "\n",
        "Our specific parameters are:\n",
        "- $u_x = 0.5$ (x-velocity)\n",
        "- $u_y = 1.0$ (y-velocity)\n",
        "- Domain: $\\Omega = [0,1] \\times [0,1]$\n",
        "- Boundary conditions: Periodic in both x and y directions\n",
        "- Initial condition: A square pulse with $q = 1.0$ in $[0.1,0.6] \\times [0.1,0.6]$ and $q = 0.1$ elsewhere\n",
        "- Grid: $50 \\times 50$ points\n",
        "- Time domain: $t \\in [0, 2.0]$\n",
        "- Number of time steps: 20\n",
        "\n",
        "## 2. Solution Representation\n",
        "\n",
        "### 3.1 Numerical Solution\n",
        "\n",
        "After solving the PDE, we obtain the solution at each grid point and time step. We organize this solution data into a matrix $Y$ with the following structure:\n",
        "\n",
        "$$Y =\n",
        "\\begin{bmatrix}\n",
        "y(x_1, y_1, t_1) & y(x_1, y_1, t_2) & \\cdots & y(x_1, y_1, t_m) \\\\\n",
        "y(x_2, y_1, t_1) & y(x_2, y_1, t_2) & \\cdots & y(x_2, y_1, t_m) \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "y(x_n, y_p, t_1) & y(x_n, y_p, t_2) & \\cdots & y(x_n, y_p, t_m)\n",
        "\\end{bmatrix}$$\n",
        "\n",
        "- Each row corresponds to a specific spatial location $(x_i, y_j)$\n",
        "- Each column corresponds to a time step $t_k$\n",
        "- The matrix has dimensions $(n \\cdot p) \\times m$ where:\n",
        "  - $n = 50$ (number of x grid points)\n",
        "  - $p = 50$ (number of y grid points)\n",
        "  - $m = 21$ (number of time steps, including initial condition)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b4e4fa79-b76a-4cc3-ab6f-04472ff0c569",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4e4fa79-b76a-4cc3-ab6f-04472ff0c569",
        "outputId": "bb973deb-76bf-45a6-8811-c3ea813ddad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Grid dimensions: 50x50\n",
            "Number of frames returned by solver: 20\n",
            "\n",
            "Y matrix shape: (2500, 21)\n",
            "\n",
            "Value at (x=10, y=10, t=0): 1.0\n",
            "Value at (x=25, y=25, t=10): 0.11861701264061458\n",
            "[[0.1        0.1        0.1        ... 0.69898093 0.19590718 0.10000336]\n",
            " [0.1        0.1        0.1        ... 0.69909167 0.21808002 0.10004222]\n",
            " [0.1        0.1        0.1        ... 0.69910662 0.2284222  0.10034163]\n",
            " ...\n",
            " [0.1        0.1        0.1        ... 0.48507539 0.10268632 0.1       ]\n",
            " [0.1        0.1        0.1        ... 0.49280699 0.1108123  0.1       ]\n",
            " [0.1        0.1        0.1        ... 0.49484006 0.12514848 0.10000006]]\n"
          ]
        }
      ],
      "source": [
        "from pde.AdvectionEquation import Advection2D, Adv2dModelConfig\n",
        "config = Adv2dModelConfig()\n",
        "adv_obj = Advection2D(config)\n",
        "# Generate initial condition\n",
        "init_cond = adv_obj.initial_condition()\n",
        "# Solve PDE\n",
        "results = adv_obj.step(init_cond)\n",
        "# Get actual dimensions\n",
        "n = adv_obj.nx  # number of x grid points\n",
        "p = adv_obj.ny  # number of y grid points\n",
        "actual_frames = len(results)  # actual number of frames returned\n",
        "print(f\"Grid dimensions: {n}x{p}\")\n",
        "print(f\"Number of frames returned by solver: {actual_frames}\")\n",
        "# Total time steps including initial condition\n",
        "m = actual_frames + 1\n",
        "# Create the Y matrix with shape (n*p, m)\n",
        "Y = np.zeros((n * p, m))\n",
        "# Fill in the initial condition (t=0)\n",
        "Y[:, 0] = init_cond.reshape(-1)\n",
        "# Fill in the remaining time steps\n",
        "for t in range(1, m):\n",
        "    # Get the solution at time step t-1 (frames are 0-indexed)\n",
        "    q_t = results[t-1]\n",
        "        # Flatten the 2D spatial grid into a column\n",
        "    Y[:, t] = q_t.reshape(-1)\n",
        "\n",
        "print(f\"\\nY matrix shape: {Y.shape}\")\n",
        "# Verify a few values from the Y matrix\n",
        "print(f\"\\nValue at (x=10, y=10, t=0): {Y[10 + 10*n, 0]}\")\n",
        "middle_t = m // 2\n",
        "print(f\"Value at (x=25, y=25, t={middle_t}): {Y[25 + 25*n, middle_t]}\")\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The covariance function  is defined in Equation (20) of the paper as:\n",
        "\n",
        "\n",
        "\n",
        "# Covariance Function and Discretization\n",
        "\n",
        "## Continuous Covariance Function R(x, ζ)\n",
        "\n",
        "The covariance function R(x, ζ) is defined in Equation (20) of the paper as:\n",
        "\n",
        "$$\\int_{\\Omega} R(x, \\zeta) \\phi_i(\\zeta) d\\zeta = \\lambda_i \\phi_i(x)$$\n",
        "\n",
        "Where:\n",
        "- R(x, ζ) = ⟨y(x, t)y(ζ, t)⟩ is the spatial two-point correlation function\n",
        "- Measures how strongly the values of y(x,t) and y(ζ,t) are correlated over time\n",
        "- φ_i(x) are the eigenfunctions of R(x, ζ), obtained from the Karhunen–Loève Decomposition (KLD)\n",
        "\n",
        "## Discretization of Covariance Matrix\n",
        "\n",
        "From Equation (24) of the paper, the temporal correlation is defined as:\n",
        "\n",
        "$$C_{tk} = \\frac{1}{l} \\int_{\\Omega} y(\\zeta, t) y(\\zeta, k) d\\zeta$$\n",
        "\n",
        "Where:\n",
        "- C_{tk} represents the temporal correlation between states at different time steps\n",
        "- The integral over Ω converts the continuous form into a discrete covariance matrix\n",
        "\n",
        "### Discrete Covariance Matrix Formulation\n",
        "\n",
        "The basic discrete form:\n",
        "\n",
        "$$R_{ij} = \\frac{1}{m} \\sum_{t=1}^{m} y(x_i, t) y(x_j, t)$$\n",
        "\n",
        "### Mean-Subtracted Covariance Matrix\n",
        "\n",
        "To center the data, use mean subtraction:\n",
        "\n",
        "$$R_{ij} = \\frac{1}{m} \\sum_{t=1}^{m} \\left( y(x_i, t) - \\bar{y}(x_i) \\right) \\left( y(x_j, t) - \\bar{y}(x_j) \\right)$$\n",
        "\n",
        "\n",
        "\n",
        "## Source\n",
        "Appendix A, Equations (20) and (24) in the referenced paper.\n"
      ],
      "metadata": {
        "id": "VwZUd_u2wlsF"
      },
      "id": "VwZUd_u2wlsF"
    },
    {
      "cell_type": "code",
      "source": [
        "from reward.reward import RewardCalculator\n",
        "# Reshape Y from (n*p, m) to (n, p, m) for RewardCalculator\n",
        "Y_3d = Y.reshape(n, p, m)\n",
        "# Create RewardCalculator instance\n",
        "reward_calc = RewardCalculator(Y_3d)\n",
        "# Compute covariance matrix\n",
        "cov_matrix = reward_calc.compute_covariance_matrix()\n",
        "print(f\"Covariance matrix shape: {cov_matrix.shape}\")\n",
        "print(f\"Expected shape: ({n*p}, {n*p})\")\n",
        "type(cov_matrix)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRijBi2uycxH",
        "outputId": "9b20a07b-008d-4c7b-a7e9-43472a4477b2"
      },
      "id": "fRijBi2uycxH",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Covariance matrix shape: (2500, 2500)\n",
            "Expected shape: (2500, 2500)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Solve the Eigenvalue Problem**\n",
        "\n",
        "decompose the covariance matrix to find dominant spatial patterns using:\n",
        "\n",
        "$$\n",
        "R \\Phi = \\Lambda \\Phi\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\Phi$ contains **eigenvectors** (KLD modes).\n",
        "- $\\Lambda$ is a **diagonal matrix of eigenvalues** (variance captured by each mode).\n",
        "- Eigenvalues are sorted in **descending order**, ensuring the most important modes are selected.\n",
        "\n",
        "**Reference:** Appendix A, Equation (20).\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HXM5grIlDevv"
      },
      "id": "HXM5grIlDevv"
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Eigenvalue Decomposition\n",
        "# Now that we've fixed the RewardCalculator class, we can use its methods directly\n",
        "\n",
        "# Solve eigenvalue problem\n",
        "eigenvalues, eigenvectors = reward_calc.solve_eigenvalue_problem()\n",
        "print(f\"Number of eigenvalues: {len(eigenvalues)}\")\n",
        "print(f\"Top 5 eigenvalues: {eigenvalues[:5]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oT4ZN3Ro4Bqt",
        "outputId": "c41607fb-e97e-49f4-a7f4-deee46ba4fcc"
      },
      "id": "oT4ZN3Ro4Bqt",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "change 3 \n",
            "\n",
            "Number of eigenvalues: 2500\n",
            "Top 5 eigenvalues: [88.7095912  67.38376706 53.2815858  47.78793793 26.32981453]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Select KLD Modes**\n",
        "\n",
        "The total **energy captured** by the first k modes is given by:\n",
        "\n",
        "$$\n",
        "E_k = \\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{j=1}^{n} \\lambda_j}\n",
        "$$\n",
        "\n",
        "We choose k such that:\n",
        "\n",
        "$$\n",
        "E_k \\geq 0.99\n",
        "$$\n",
        "\n",
        "This ensures we retain **99% of the system's variance**.\n",
        "\n",
        "**Reference:** Appendix A, Equation (25)"
      ],
      "metadata": {
        "id": "N87hg9TXDrEa"
      },
      "id": "N87hg9TXDrEa"
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Mode Selection\n",
        "# Calculate cumulative energy\n",
        "cumulative_energy = np.cumsum(eigenvalues) / np.sum(eigenvalues)\n",
        "threshold = 0.99\n",
        "num_modes = np.searchsorted(cumulative_energy, threshold) + 1\n",
        "print(f\"Number of modes required to capture {threshold*100}% of energy: {num_modes}\")\n",
        "# Select KLD modes\n",
        "selected_modes = reward_calc.select_KLD_modes(num_modes)\n",
        "print(f\"Selected modes shape: {selected_modes.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53iwXuGEDrf_",
        "outputId": "2e96846e-24fc-4048-d206-4a12b7287005"
      },
      "id": "53iwXuGEDrf_",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of modes required to capture 99.0% of energy: 14\n",
            "Selected modes shape: (2500, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **4. Compute the Reward Function**\n",
        "\n",
        "The **reward function** is based on the **information gain** from sensor placement.\n",
        "\n",
        "1. **Extract the sensor locations** from the selected KLD modes:\n",
        "\n",
        "$$\n",
        "P_m = \\text{selected\\_modes}[ \\text{sensor\\_indices}, :]\n",
        "$$\n",
        "\n",
        "2. **Compute the reduced observability matrix**:\n",
        "\n",
        "$$\n",
        "T_m = P_m^T P_m\n",
        "$$\n",
        "\n",
        "3. **Define the reward as the determinant**:\n",
        "\n",
        "$$\n",
        "\\text{Reward} = \\log \\det(T_m)\n",
        "$$\n",
        "\n",
        "Equation(12)"
      ],
      "metadata": {
        "id": "yZq43PRwD9Ul"
      },
      "id": "yZq43PRwD9Ul"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. Reward Calculation\n",
        "# Calculate reward for a sample set of sensor positions\n",
        "example_sensors = [(25, 25), (10, 40), (30, 15)]\n",
        "flat_indices = [i + j * n for i, j in example_sensors]\n",
        "reward = reward_calc.compute_reward_function(flat_indices)\n",
        "print(f\"Reward for KLD Method: {reward}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWF12KCpD9oL",
        "outputId": "67558af5-c4b3-48b1-b0f0-7e0f039ec387"
      },
      "id": "RWF12KCpD9oL",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reward for KLD Method: 1.1150733081682884e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from reward.reward import RewardCalculator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(f\"PDE solution data shape: {Y_3d.shape}\")\n",
        "\n",
        "# Create a RewardCalculator instance\n",
        "reward_calc = RewardCalculator(Y_3d)\n",
        "\n",
        "# Compute KLD using the original approach\n",
        "print(\"Computing KLD using original approach...\")\n",
        "cov_matrix = reward_calc.compute_covariance_matrix()\n",
        "eigenvalues_orig, eigenvectors_orig = reward_calc.solve_eigenvalue_problem()\n",
        "\n",
        "# Determine number of modes to keep (99% energy)\n",
        "cumulative_energy_orig = np.cumsum(eigenvalues_orig) / np.sum(eigenvalues_orig)\n",
        "threshold = 0.99\n",
        "num_modes_orig = np.searchsorted(cumulative_energy_orig, threshold) + 1\n",
        "print(f\"Number of modes required to capture {threshold*100}% of energy (original): {num_modes_orig}\")\n",
        "\n",
        "# Select KLD modes\n",
        "selected_modes_orig = reward_calc.select_KLD_modes(num_modes_orig)\n",
        "print(f\"Selected modes shape (original): {selected_modes_orig.shape}\")\n",
        "\n",
        "print(\"\\nComputing KLD using PCA-based approach...\")\n",
        "selected_modes_pca, eigenvalues_pca, num_modes_pca = reward_calc.compute_kld_with_pca(threshold)\n",
        "print(f\"Number of modes required to capture {threshold*100}% of energy (PCA): {num_modes_pca}\")\n",
        "print(f\"Selected modes shape (PCA): {selected_modes_pca.shape}\")\n",
        "print(\"\\nComparing top 5 eigenvalues:\")\n",
        "print(f\"Original approach: {eigenvalues_orig[:5]}\")\n",
        "print(f\"PCA approach: {eigenvalues_pca[:5]}\")\n",
        "rel_diff = np.abs(eigenvalues_orig[:num_modes_orig] - eigenvalues_pca[:num_modes_orig]) / eigenvalues_orig[:num_modes_orig]\n",
        "print(f\"\\nMean relative difference in eigenvalues: {np.mean(rel_diff):.6f}\")\n",
        "print(\"\\nComparing rewards for sample sensor positions:\")\n",
        "example_sensors = [(25, 25), (10, 40), (30, 15)]\n",
        "flat_indices = [i + j * n for i, j in example_sensors]\n",
        "reward_orig = reward_calc.compute_reward_function(flat_indices)\n",
        "reward_pca = reward_calc.compute_reward_function_pca(flat_indices)\n",
        "\n",
        "print(f\"Reward for KLD Method (original): {reward_orig}\")\n",
        "print(f\"Reward for KLD Method (PCA): {reward_pca}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cAvL7bj7WW2",
        "outputId": "2acbf1a4-1757-4734-c893-76af9a184ab7"
      },
      "id": "8cAvL7bj7WW2",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDE solution data shape: (50, 50, 21)\n",
            "Computing KLD using original approach...\n",
            "change 3 \n",
            "\n",
            "Number of modes required to capture 99.0% of energy (original): 14\n",
            "Selected modes shape (original): (2500, 14)\n",
            "\n",
            "Computing KLD using PCA-based approach...\n",
            "Number of modes required to capture 99.0% of energy (PCA): 14\n",
            "Selected modes shape (PCA): (2500, 14)\n",
            "\n",
            "Comparing top 5 eigenvalues:\n",
            "Original approach: [88.7095912  67.38376706 53.2815858  47.78793793 26.32981453]\n",
            "PCA approach: [88.7095912  67.38376706 53.2815858  47.78793793 26.32981453]\n",
            "\n",
            "Mean relative difference in eigenvalues: 0.000000\n",
            "\n",
            "Comparing rewards for sample sensor positions:\n",
            "Reward for KLD Method (original): 1.1150733081682884e-07\n",
            "Reward for KLD Method (PCA): 1.1150733081682884e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\"\n",
        "# 6. Find Optimal Single Sensor Location\n",
        "print(\"\\nFinding optimal single sensor location...\")\n",
        "best_reward = -float('inf')\n",
        "best_location = None\n",
        "\n",
        "# Sample grid points to reduce computation (every 5th point)\n",
        "step = 5\n",
        "grid_samples = [(i, j) for i in range(0, n, step) for j in range(0, p, step)]\n",
        "print(f\"Testing {len(grid_samples)} locations...\")\n",
        "\n",
        "for i, j in grid_samples:\n",
        "    flat_idx = i + j * n\n",
        "    current_reward = reward_calc.compute_reward_function([flat_idx])\n",
        "\n",
        "    if current_reward > best_reward:\n",
        "        best_reward = current_reward\n",
        "        best_location = (i, j)\n",
        "\n",
        "print(f\"Best single sensor location: {best_location}\")\n",
        "print(f\"Reward value: {best_reward}\")\"\"\""
      ],
      "metadata": {
        "id": "yvTmTEY2Fj_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "f86ab5e6-e66a-4c13-b6ad-a2932fc23d8a"
      },
      "id": "yvTmTEY2Fj_S",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\"\\n# 6. Find Optimal Single Sensor Location\\nprint(\"\\nFinding optimal single sensor location...\")\\nbest_reward = -float(\\'inf\\')\\nbest_location = None\\n\\n# Sample grid points to reduce computation (every 5th point)\\nstep = 5\\ngrid_samples = [(i, j) for i in range(0, n, step) for j in range(0, p, step)]\\nprint(f\"Testing {len(grid_samples)} locations...\")\\n\\nfor i, j in grid_samples:\\n    flat_idx = i + j * n\\n    current_reward = reward_calc.compute_reward_function([flat_idx])\\n\\n    if current_reward > best_reward:\\n        best_reward = current_reward\\n        best_location = (i, j)\\n\\nprint(f\"Best single sensor location: {best_location}\")\\nprint(f\"Reward value: {best_reward}\")'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from GymEnvironment.environment2 import SensorOptimalPlacement\n",
        "\n",
        "\n",
        "def run_random_policy_benchmark(\n",
        "        width=50,\n",
        "        length=50,\n",
        "        n_sensor=5,\n",
        "        seed=42,\n",
        "        num_episodes=10,\n",
        "        max_horizon = 20,\n",
        "        N_max = 5,\n",
        "):\n",
        "    # Create an instance of your custom env\n",
        "    env = SensorOptimalPlacement(width=width, length=length, n_sensor=n_sensor, max_horizon= max_horizon, N_max= N_max, seed=seed)\n",
        "\n",
        "    all_episode_rewards = []\n",
        "\n",
        "    \"\"\"\n",
        "    for episode_idx in range(num_episodes):\n",
        "        # Reset environment and get initial observation\n",
        "        obs = env.reset(seed = episode_idx)\n",
        "\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            # Sample a random action\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "            # Take a step in the environment\n",
        "            obs, reward, done, info = env.step(action)\n",
        "\n",
        "            # Check for a terminal condition (if your env supports it)\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Store this episode's reward\n",
        "        # all_episode_rewards.append(episode_reward)\n",
        "        # print(f\"Episode {episode_idx + 1}/{num_episodes} - Reward: {episode_reward}\")\n",
        "\n",
        "    # Print or return some statistics\n",
        "    avg_reward = np.mean(all_episode_rewards)\n",
        "    print(f\"\\nCompleted {num_episodes} episodes using random policy.\")\n",
        "    print(f\"Average reward: {avg_reward:.4f}\")\n",
        "    print(f\"Reward per episode: {all_episode_rewards}\")\n",
        "\n",
        "    return all_episode_rewards\"\"\"\n",
        "    best_rewards = []\n",
        "    for episode_idx in range(num_episodes):\n",
        "        # Reset environment and get initial observation\n",
        "        obs = env.reset(seed=episode_idx)\n",
        "        episode_rewards = []\n",
        "        done = False\n",
        "        truncated = False\n",
        "        print(f\"Starting episode {episode_idx + 1}/{num_episodes}\")\n",
        "        step = 0\n",
        "        while not done and not truncated:\n",
        "            action = env.action_space.sample()\n",
        "            obs, reward, done, truncated, info = env.step(action)\n",
        "            episode_rewards.append(reward)\n",
        "            step += 1\n",
        "            print(f\"  Step {step}, Current reward: {reward:.6f}\")\n",
        "        best_rewards.append(info[\"max_reward\"])\n",
        "        all_episode_rewards.append(episode_rewards)\n",
        "        print(f\"Episode {episode_idx + 1}/{num_episodes} complete - Max Reward: {info['max_reward']:.6f}\")\n",
        "    print(f\"\\nCompleted {num_episodes} episodes using random policy.\")\n",
        "    print(f\"Best reward overall: {max(best_rewards):.6f}\")\n",
        "    print(f\"Average best reward per episode: {np.mean(best_rewards):.6f}\")\n",
        "\n",
        "    return all_episode_rewards, best_rewards\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run the benchmark with random actions\n",
        "    run_random_policy_benchmark()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whPKj6ZwQdwC",
        "outputId": "b7718df6-76af-4813-a5f9-05c29d9b7841"
      },
      "id": "whPKj6ZwQdwC",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting episode 1/10\n",
            "  Step 1, Current reward: -32.234410\n",
            "  Step 2, Current reward: -32.071594\n",
            "  Step 3, Current reward: -31.869149\n",
            "  Step 4, Current reward: -29.030027\n",
            "  Step 5, Current reward: -29.239563\n",
            "  Step 6, Current reward: -30.483115\n",
            "  Step 7, Current reward: -28.744890\n",
            "  Step 8, Current reward: -27.467946\n",
            "  Step 9, Current reward: -27.055600\n",
            "  Step 10, Current reward: -26.214720\n",
            "  Step 11, Current reward: -26.466955\n",
            "  Step 12, Current reward: -26.612760\n",
            "  Step 13, Current reward: -26.450141\n",
            "  Step 14, Current reward: -26.237444\n",
            "  Step 15, Current reward: -26.121153\n",
            "  Step 16, Current reward: -26.397831\n",
            "  Step 17, Current reward: -25.997520\n",
            "  Step 18, Current reward: -26.636314\n",
            "  Step 19, Current reward: -26.568679\n",
            "  Step 20, Current reward: -26.650465\n",
            "  Step 21, Current reward: -26.868422\n",
            "Episode 1/10 complete - Max Reward: -25.997520\n",
            "Starting episode 2/10\n",
            "  Step 1, Current reward: -28.297287\n",
            "  Step 2, Current reward: -28.031119\n",
            "  Step 3, Current reward: -28.560967\n",
            "  Step 4, Current reward: -27.082952\n",
            "  Step 5, Current reward: -29.388590\n",
            "  Step 6, Current reward: -29.249147\n",
            "  Step 7, Current reward: -30.871791\n",
            "  Step 8, Current reward: -27.877791\n",
            "  Step 9, Current reward: -27.692143\n",
            "  Step 10, Current reward: -27.474652\n",
            "Episode 2/10 complete - Max Reward: -27.082952\n",
            "Starting episode 3/10\n",
            "  Step 1, Current reward: -26.840317\n",
            "  Step 2, Current reward: -26.829726\n",
            "  Step 3, Current reward: -26.470155\n",
            "  Step 4, Current reward: -25.920844\n",
            "  Step 5, Current reward: -26.658448\n",
            "  Step 6, Current reward: -26.687378\n",
            "  Step 7, Current reward: -27.038819\n",
            "  Step 8, Current reward: -27.186897\n",
            "  Step 9, Current reward: -30.874360\n",
            "  Step 10, Current reward: -30.508418\n",
            "Episode 3/10 complete - Max Reward: -25.920844\n",
            "Starting episode 4/10\n",
            "  Step 1, Current reward: -27.863205\n",
            "  Step 2, Current reward: -28.191413\n",
            "  Step 3, Current reward: -28.331448\n",
            "  Step 4, Current reward: -28.229851\n",
            "  Step 5, Current reward: -26.620261\n",
            "  Step 6, Current reward: -25.789358\n",
            "  Step 7, Current reward: -25.711393\n",
            "  Step 8, Current reward: -26.676773\n",
            "  Step 9, Current reward: -26.046816\n",
            "  Step 10, Current reward: -26.138943\n",
            "  Step 11, Current reward: -26.906983\n",
            "  Step 12, Current reward: -29.047062\n",
            "  Step 13, Current reward: -28.950170\n",
            "Episode 4/10 complete - Max Reward: -25.711393\n",
            "Starting episode 5/10\n",
            "  Step 1, Current reward: -28.875985\n",
            "  Step 2, Current reward: -29.245793\n",
            "  Step 3, Current reward: -26.253745\n",
            "  Step 4, Current reward: -25.887860\n",
            "  Step 5, Current reward: -25.916411\n",
            "  Step 6, Current reward: -26.038780\n",
            "  Step 7, Current reward: -26.291092\n",
            "  Step 8, Current reward: -26.749322\n",
            "  Step 9, Current reward: -27.291084\n",
            "  Step 10, Current reward: -26.873411\n",
            "Episode 5/10 complete - Max Reward: -25.887860\n",
            "Starting episode 6/10\n",
            "  Step 1, Current reward: -26.681155\n",
            "  Step 2, Current reward: -26.619202\n",
            "  Step 3, Current reward: -26.727208\n",
            "  Step 4, Current reward: -26.354457\n",
            "  Step 5, Current reward: -25.901698\n",
            "  Step 6, Current reward: -26.407531\n",
            "  Step 7, Current reward: -26.423772\n",
            "  Step 8, Current reward: -27.336474\n",
            "  Step 9, Current reward: -26.808145\n",
            "  Step 10, Current reward: -26.325132\n",
            "  Step 11, Current reward: -26.231741\n",
            "Episode 6/10 complete - Max Reward: -25.901698\n",
            "Starting episode 7/10\n",
            "  Step 1, Current reward: -25.965074\n",
            "  Step 2, Current reward: -25.962441\n",
            "  Step 3, Current reward: -26.057913\n",
            "  Step 4, Current reward: -33.179380\n",
            "  Step 5, Current reward: -32.761876\n",
            "  Step 6, Current reward: -33.147523\n",
            "  Step 7, Current reward: -26.415018\n",
            "  Step 8, Current reward: -26.088410\n",
            "Episode 7/10 complete - Max Reward: -25.962441\n",
            "Starting episode 8/10\n",
            "  Step 1, Current reward: -27.084141\n",
            "  Step 2, Current reward: -26.802838\n",
            "  Step 3, Current reward: -26.572639\n",
            "  Step 4, Current reward: -25.896756\n",
            "  Step 5, Current reward: -27.301436\n",
            "  Step 6, Current reward: -27.461311\n",
            "  Step 7, Current reward: -27.191213\n",
            "  Step 8, Current reward: -26.818015\n",
            "  Step 9, Current reward: -26.821982\n",
            "  Step 10, Current reward: -27.217338\n",
            "Episode 8/10 complete - Max Reward: -25.896756\n",
            "Starting episode 9/10\n",
            "  Step 1, Current reward: -29.483581\n",
            "  Step 2, Current reward: -29.881535\n",
            "  Step 3, Current reward: -28.851047\n",
            "  Step 4, Current reward: -28.344909\n",
            "  Step 5, Current reward: -28.440501\n",
            "  Step 6, Current reward: -28.397648\n",
            "  Step 7, Current reward: -28.043951\n",
            "  Step 8, Current reward: -33.714054\n",
            "  Step 9, Current reward: -33.582454\n",
            "  Step 10, Current reward: -27.424381\n",
            "  Step 11, Current reward: -28.441020\n",
            "  Step 12, Current reward: -29.953854\n",
            "  Step 13, Current reward: -28.949796\n",
            "  Step 14, Current reward: -28.458564\n",
            "  Step 15, Current reward: -28.332830\n",
            "  Step 16, Current reward: -27.679724\n",
            "Episode 9/10 complete - Max Reward: -27.424381\n",
            "Starting episode 10/10\n",
            "  Step 1, Current reward: -27.479988\n",
            "  Step 2, Current reward: -26.840299\n",
            "  Step 3, Current reward: -27.397748\n",
            "  Step 4, Current reward: -27.378934\n",
            "  Step 5, Current reward: -30.173282\n",
            "  Step 6, Current reward: -27.443234\n",
            "  Step 7, Current reward: -28.585247\n",
            "  Step 8, Current reward: -30.978763\n",
            "Episode 10/10 complete - Max Reward: -26.840299\n",
            "\n",
            "Completed 10 episodes using random policy.\n",
            "Best reward overall: -25.711393\n",
            "Average best reward per episode: -26.262615\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}